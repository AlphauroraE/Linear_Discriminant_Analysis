---
title: "QW_LDA_Scratch"
output: html_document
date: "2023-11-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# My own LDA Function:
lda_function = function(data,data_labels){
  # This function contains the univariate version of the LDA function.
  # We need a few things in order to estimate the mean of each class, shared variance, and prior probabilities.
  
  #n = number of data points there are total
  n = length(data_labels)
  
  classes = unique(data_labels)
  num_classes = length(classes)
  
  # Initialize mean of each classes, shared variance, and prior probabilities
  mu_hat = numeric(num_classes)
  pi_hat = numeric(num_classes)
  variance = numeric(1)
  variance[1] = 0
  sum_xs = numeric(1)
  sum_xs[1] = 0
  
  # Find the mean and prior probability of each class:
  for (k in 1:num_classes){
    n_k = sum(data_labels == classes[k])
    
    pi_hat[k] = n_k/n
    
    sum_xs[1] = 0
    for (i in 1:n){
      if (data_labels[i] == classes[k]){
        sum_xs = sum_xs + data[i]
      }
    }
    mu_hat[k] = sum_xs/n_k
  }
  
  # Find the shared variance:
  for (k in 1:num_classes){
    for (i in 1:n){
      if (data_labels[i] == classes[k]){
        data_minus_mean = data[i] - mu_hat[k]
        dmm_squared = data_minus_mean^2
        variance = variance + dmm_squared
      }
    }
  }
  variance = variance/(n-num_classes)
  
  # Plug into the discriminant function to generate predicted labels
  discriminants = matrix(0,nrow = n,ncol = num_classes)
  
  for (k in 1:num_classes){
    for (i in 1:n){
      x = data[i]
      discriminants[i,k] = (x * (mu_hat[k]/variance)) - ((mu_hat[k]^2)/(2*variance)) + log(pi_hat[k])
    }
  }
  
  # Go row by row in the "discriminants" matrix and identify the max value per row (which class has the larger discriminant)
  # The predicted label is the class with the larger discriminant.
  predicted_labels = apply(discriminants,1,which.max)
    
    
  return(predicted_labels)
}
```



```{r}
# The following is code written by my professor
univariate_lda_scratch = function(data, class_labels) {
  
  N = length(class_labels)  # Number of observations
  
  unique_classes = unique(class_labels) 
  num_classes = length(unique_classes)  # Number of classes
  
  pi_hat = numeric(num_classes)  # Store the class's sample prior probability
  mu_hat = numeric(num_classes)  # Store the class's sample mean
  sigma_hat = numeric(1)  # Store the pooled (common) sample variance of classes
  
  for (k in 1:num_classes) {
    class_k = unique_classes[k]
    Nk = sum(class_labels == class_k)  # Get the number of observations for each class
    
    # Estimate prior probability πˆk
    pi_hat[k] = Nk / N
    
    # Estimate mean µˆk
    mu_hat[k] = mean(data[class_labels == class_k])
    
    # Estimate common variance σˆ for two classes
    for (i in 1:N) {
      if (class_labels[i] == class_k) {
        xi_minus_mu = data[i] - mu_hat[k]
        sigma_hat = sigma_hat + xi_minus_mu^2
      }
    }
  }
  sigma_hat = sigma_hat / (N - num_classes)
  
  # Calculate discriminant function values
  discriminants = matrix(0, nrow = N, ncol = num_classes)
  for (i in 1:N) {
    for (k in 1:num_classes) {
      discriminants[i, k] = data[i] * (1 / sigma_hat) * mu_hat[k] - (1 / (2 * sigma_hat)) * (mu_hat[k]^2) + log(pi_hat[k])
    }
  }
  
  # Make class decisions for all data points
  predicted_classes = apply(discriminants, 1, which.max)
  
  return(predicted_classes)
}
```



```{r}
# Test the function:
credit = read.table("SouthGermanCredit.asc",header=TRUE)
head(credit)
data = credit$laufzeit #laufzeit means duration
data_labels = credit$kredit #kredit means credit_risk

# Testing my own LDA Function:
predicted_labels = lda_function(data,data_labels)
head(predicted_labels)

# Calculate the MSE
mean((data_labels - predicted_labels)^2)

# Comparing to professor's code
predicted_labels = univariate_lda_scratch(data,data_labels)
head(predicted_labels)
mean((data_labels - predicted_labels)^2)
```































